{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low: 0 to 68295.68000000001\n",
      "Medium: 68295.68000000001 to 189775.28\n",
      "High: 189775.28 and above\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 77\u001b[0m\n\u001b[0;32m     75\u001b[0m     image_paths\u001b[38;5;241m.\u001b[39mappend(img_path)\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43mextract_sift_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m sift_features_list\u001b[38;5;241m.\u001b[39mappend(features)\n\u001b[0;32m     79\u001b[0m image_paths\u001b[38;5;241m.\u001b[39mappend(img_path)\n",
      "Cell \u001b[1;32mIn[2], line 10\u001b[0m, in \u001b[0;36mextract_sift_features\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_sift_features\u001b[39m(image_path):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;66;03m# Read the image in grayscale\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m         image \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIMREAD_GRAYSCALE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m image \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     12\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to load image \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Function to extract SIFT features\n",
    "def extract_sift_features(image_path):\n",
    "    try:\n",
    "        # Read the image in grayscale\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if image is None:\n",
    "            print(f\"Failed to load image {image_path}\")\n",
    "            return np.zeros(500)  # Return zero vector if image cannot be loaded\n",
    "        \n",
    "        # Initialize SIFT detector\n",
    "        sift = cv2.SIFT_create()\n",
    "        \n",
    "        # Detect keypoints and compute descriptors\n",
    "        keypoints, descriptors = sift.detectAndCompute(image, None)\n",
    "        \n",
    "        if descriptors is not None:\n",
    "            # Flatten the descriptors into a 1D array\n",
    "            sift_features = descriptors.flatten()\n",
    "            \n",
    "            # Limit the size of the feature vector\n",
    "            max_features = 500\n",
    "            if sift_features.size < max_features:\n",
    "                # Pad with zeros\n",
    "                sift_features = np.pad(sift_features, (0, max_features - sift_features.size), 'constant')\n",
    "            else:\n",
    "                # Truncate\n",
    "                sift_features = sift_features[:max_features]\n",
    "            return sift_features\n",
    "        else:\n",
    "            # Return zero vector if no descriptors are found\n",
    "            return np.zeros(500)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return np.zeros(500)\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv('../data/instagram_data.csv')\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Convert 'likes' into 3 classes\n",
    "bins = [0, 100000, 200000, np.inf]\n",
    "labels = ['Low', 'Medium', 'High']\n",
    "data['likes_class'] = pd.cut(data['likes'], bins=bins, labels=labels)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "data['likes_class_encoded'] = le.fit_transform(data['likes_class'])\n",
    "\n",
    "# Extract SIFT features for all images\n",
    "sift_features_list = []\n",
    "image_paths = []\n",
    "\n",
    "for idx, row in data.iterrows():\n",
    "    img_path = row['image_path']\n",
    "    if not os.path.exists(img_path):\n",
    "        print(f\"File not found: {img_path}\")\n",
    "        sift_features_list.append(np.zeros(500))\n",
    "        image_paths.append(img_path)\n",
    "        continue\n",
    "    features = extract_sift_features(img_path)\n",
    "    sift_features_list.append(features)\n",
    "    image_paths.append(img_path)\n",
    "\n",
    "# Create DataFrame of features\n",
    "sift_features_df = pd.DataFrame(sift_features_list)\n",
    "\n",
    "# Combine features with original data\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "sift_features_df.reset_index(drop=True, inplace=True)\n",
    "combined_data = pd.concat([data, sift_features_df], axis=1)\n",
    "\n",
    "# Prepare features and target\n",
    "exclude_columns = ['likes', 'no_of_comments', 't', 'follower_count_at_t', 'image_path', 'likes_class', 'likes_class_encoded']\n",
    "feature_columns = [col for col in combined_data.columns if col not in exclude_columns]\n",
    "X = combined_data[feature_columns]\n",
    "X.columns = X.columns.map(str)\n",
    "X.fillna(0, inplace=True)\n",
    "y = combined_data['likes_class_encoded']\n",
    "\n",
    "# Split dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Train Random Forest model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=labels))\n",
    "\n",
    "# Confusion Matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens',\n",
    "            xticklabels=labels, yticklabels=labels)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FPkernal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
